{
	"quant_method": "sparseml",
	"format": "fakequant",
	"quantization_status": "frozen",
	"global_compression_ratio": null,
	"config_groups": {
        "group_1": {
            "weights": {
                "num_bits": 8,
                "type": "int",
                "symmetric": true,
                "strategy": "tensor"
            },
            "input_activations": {
                "num_bits": 8,
                "type": "int",
                "symmetric": true,
                "strategy": "tensor"
            },
            "targets": ["Linear"]
        },
        "group_2": {
            "weights": {
                "num_bits": 8,
                "type": "int",
                "symmetric": false,
                "strategy": "tensor"
            },
            "input_activations": null,
            "targets": ["Embedding"]
        }
    },
	"ignore": [
        "LlamaRotaryEmbedding", "LlamaRMSNorm", "SiLUActivation", 
        "model.layers.1.mlp.down_proj", "MatMulLeftInput_QK", "MatMulRightInput_QK",
        "MatMulOutput_QK", "MatMulLeftInput_PV", "MatMulRightInput_PV", 
        "MatMulOutput_PV"
    ]
}